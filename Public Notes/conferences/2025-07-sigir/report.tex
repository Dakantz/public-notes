\documentclass[a4paper]{article}
\usepackage[utf8x]{inputenc}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{longtable}

\usepackage{hyperref}

\usepackage{tabularray}
\usepackage{graphicx}

\usepackage[table]{xcolor}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{url}
\usepackage[style=ieee,backend=biber]{biblatex} % Bibliography
\usepackage{isomath}
\usepackage{amsmath}
\usepackage{newtxmath}
\usepackage{listings}
\usepackage{float}
\usepackage{bm}
\usepackage{ stmaryrd }

\usepackage{geometry}
\geometry{
    a4paper,
    left=25mm,
    right=25mm,
    top=25mm,
    }
    
\title{SIGIR '25 notes and interesting posters}
\author{Benedikt Kantz}
\date{July 13-18 2025}
\begin{document}

\maketitle

\section{Sunday}
\subsection{RAG vs. Longcontext LM Retrieval}
\url{https://sigir2025.dei.unipd.it/detailed-program/paper?paper=f6185f0ef02dcaec414a3171cd01c697}
\begin{itemize}
    \item LLamaIndex - framework for RAG Retrieval
    \item Summarization based Retrieval
          \begin{itemize}
              \item Tree/Summarization based retrieval - construct trees and traverse by encoding tree structure
              \item \enquote{RAPTOR} - flatten tree, search all nodes
          \end{itemize}
    \item Long Contex LLMs
          \begin{itemize}
              \item Main challenge: training time $\mathcal{O}(n^2), n$ tokens
              \item Extend also frequency encoding!
          \end{itemize}
    \item focuses on LM bascics with long contexts in first half
\end{itemize}
\subsection{Efficient In-Memory Inverted Indexes: Theory and Practice}
\url{https://sigir2025.dei.unipd.it/detailed-program/paper?paper=b38e5ff5f816ac6e4169bce9314b2996}
\begin{itemize}
    \item Scoring/retrieval based on scores and terms
    \item Priming index (TODO- slides available)
    \item Quantization
    \item Reordering
    \item stoping (stopwords)
    \item Efficient Index search - dynamic pruning: WAND (weak AND) good writeup: \url{https://www.elastic.co/blog/faster-retrieval-of-top-hits-in-elasticsearch-with-block-max-wand}
          \begin{itemize}
              \item Need to sort cursor when iterating (TODO: investigate further)
              \item Or sort once: MaxScore, only keep relevant docs for score by precomputed upper bound for term - all combinations have combined upper bounds!
              \item Can even be broken down to blocks - at the cost of storage
              \item Ranker is dependent on search algorithm (e.g. BM25, LM-based, ...)
              \item Threshold priming - guess initial threshold, skip docs that might not be relevant prior
          \end{itemize}
    \item Sparse retrieval
          \begin{itemize}
              \item Documents are naturally sparse - exploited through inverted indices, terms are related to only a few documents (except for stopwords - just remove them) -- same for queries! (\enquote{Zipf's Law})
              \item Exploited through TF-IDF scoring (classics), not perfect - scoring inbalanced, lexical mismatch (semantic mismatch - e.g. F1 vs Formula One)
              \item[$\implies$] learn term weighting, sparsity, ...
              \item e.g. BERT for token re-weighting -- learn important tokens
              \item Expansion -- learn new tokens to add to query, or Document Queries (i.e. Doc2Query); find similar words by using the masked prediction capabilities of BERT
              \item Then: mask again, as LMs usually very dense!
              \item In Practice: Query Weighting + Document Masked Expansion + Regularisation/top-k
              \item But: this learned indices: distributions changes, algorithms become slower  -- slower retrieval, sparsity reduces significantly
              \item[$\implies$] improve method by:
                  \begin{itemize}
                      \item Combine traditonal BM25 retrieval for index + scoring from learned models  (can even be interpolated!)
                  \end{itemize}
          \end{itemize}
    \item boolean retrieval (with inverted indexes) vs. kNN/ANN (image, multimodal, semantic, ...) -- future quite open!
\end{itemize}
\subsection{Neural Lexical Search with Learned Sparse Retrieval}
\url{https://sigir2025.dei.unipd.it/detailed-program/paper?paper=3bd4017318837e92a66298c7855f4427}
$\implies$ seems interesting, no time (all tutorials are interesting this afternoon!) :/

\subsection{Psychological Aspects in Retrieval and Recommendation}
\url{https://sigir2025.dei.unipd.it/detailed-program/paper?paper=110eec23201d80e40d0c4a48954e2ff5}
Slides \url{https://github.com/aisocietylab/Psy-IR-RecSys-SIGIR25/}
\begin{itemize}
    \item IR systems integrated in  life - play role in life - psychological impact (cognition, decision, ...)!
    \item[$\implies$] model human using emotions/personality/overload/mood (term: psychological states!)
    \item \enquote{Cognitive architecture}: first IR paper in cognitive science journal (by CS researchers!) - go back to roots
    \item higher level/lower level processes (e.g. attention)
    \item Key processes
          \begin{itemize}
              \item Attention: pull focus on key elements
              \item Memory: memorize focused elements $\implies$ pull focus
              \item Decision-making: (ir-)rationality, heuristic-based
          \end{itemize}
    \item Map processes between IR/RS scenarios

          \begin{tabular}{|r|l|}
              \hline
              IR/RS Task                          & Cognitive Process           \\
              \hline
              Query (Re-)formulation              & Working memory, attention   \\
              Ranking, Interface Design           & Attention, perception       \\
              Session continuation or abandonment & Episodic memory, fatigue    \\
              Item selection                      & Decision-making, heuristics \\
              Personalization (over time)         & Long-term memory, learning  \\
              \hline
          \end{tabular}
    \item Cognitive architectures: are \enquote{computation frameworks} -- helps us to apply models \& test them, align models in IR with users!
    \item Symbolic architecture
          \begin{itemize}
              \item Discrete symbols, with rules operating on knowledge
              \item[$\implies$] explainable, explicitly encoded; hard to expand due to handcrafted rules
          \end{itemize}
    \item Emergent/Connectionist architectures
          \begin{itemize}
              \item across network/graph, parallel processing, nodes represent subsymbolic elements
              \item[$\implies$] not (or limited) explainabilty!, a lot of data needed
          \end{itemize}
    \item Hybrid: more flexbility
    \item Survey: \url{https://link.springer.com/article/10.1007/s10462-018-9646-y#Fig3}
    \item ACT-R: Adaptive Control of Thought  -- Rational (ACT-R): memory types (a la von Neumann), with rules indside
    \item Application: recency bias in retrieval, e.g. music retrieval for non-mainstream music
    \item SOAR: goal oriented, episodic memory -- agentic framework?
    \item CLARION: hybrid
    \item LIDA: Global Workspace Theory, consciousness vs unconscious focus
    \item Problems: difficult to integrate in fuzzy pipelines like DL models, quite interdisciplinary ...
    \item Cognitive Biases
          \begin{itemize}
              \item biases play against rational (logical and optimal decisions)
              \item influences in algorithmic decision makings; encoded in various tasks, LM models (generative AND embedding!)
              \item Feature-Positive Effect: not aware of \emph{missing} things: which information is ignored; e.g. which aspects of a job RecSys are ignored?
              \item Social/Cultural Homophily: users with similar traits like similar things: experiments show users like music from their own country.
              \item Conformity bias: conform to opinion of others; e.g. show users ratings of other users first biases -- could be used to lead towards higher/lower ratings?
              \item Declinism: belief, world was a better place -- found in increase in negative terms in music lyrics
              \item Primacy/Recency Bias
          \end{itemize}
    \item Personality and affect
          \begin{itemize}
              \item States \& Traits
              \item \enquote{Affective Computing}: detect emotions from metadata/interactions
          \end{itemize}
\end{itemize}
\section{Monday}
\subsection{BM25 and All That}
\begin{itemize}
    \item History of IR, back to Cranfield experiments, first probablistic theories and how they theorized/implemented BM1-> BM25
\end{itemize}
\subsection{Multimodal Retrieval}
\subsubsection{DePro: Domain Ensemble using Decoupled Prompts for Universal Cross-Domain Retrieval}
\begin{itemize}
    \item Prompt tuning/embedding optimisation for concatinated prompts
\end{itemize}
\subsection{Reproducibility in Domain-Specific, Multimodal, ... Retrieval}
(the critic's section)
\subsubsection{Benchmark Granularity}
\begin{itemize}
    \item Image-Text: bidirectional problem
    \item Problem: benchmarks very curated/clean, real-world: noisy, ...
    \item Eval: shuffle words, different granularities of descriptions
    \item Results:
    \begin{itemize}
        \item         performance increases if queries are more fine-grained $\implies$ miss out on performance of model, use multiple granularities in benchmarks!
        \item perturbations usually hurt
    \end{itemize}
    \item Very inconsistent accross models/datasets!
\end{itemize}
\subsubsection{Visual Document Retrieval + Late Interactions}
\begin{itemize}
    \item ViT: patch-based encoding of document (ColPali/ColQwen2 \url{https://huggingface.co/vidore/colqwen2-v1.0}) using ColBERT paradigm (offline search using MaxSim \url{https://github.com/stanford-futuredata/ColBERT})
    \item Image-Documents has less loss in performance even in big index sizes compared to OCR
    \item Reason: number of patches/text tokens factor is relevant
    \item Matching to visual tokens to query is high -- seems effective 
\end{itemize}
\subsubsection{Assessing effective token length of multimodal retrieval}
\begin{itemize}
    \item Information Alignment: CLIP has a modality gap between image and text!
    \item Comes from short/descriptive captions
    \item Long-Text performance/beyond token budget using pooling?
    \item Domain-Specific Datasets
    \item[$\implies$] no model could use full token budget!, also variance between datasets
    \item How was the token length varied? removing adjectives, just cut? -- was just cut... (maybe information afterwards not relevant/good?)
\end{itemize}
\subsubsection{Refined Medical Search via Dense Retrieval and User Interaction}
Interesting for A+CHIS/HEREDITARY search engine?
\begin{itemize}
    \item Enhance User Experience with Interaction integration in Dense (embedding) Retrieval
    \item TripClick IR Dataset -> A+CHIS?
    \item Fine tune PubMedBERT on TripClick Triplets (Document, D+, D- ) -- what are D+/D-
    \item Retrieval by embedding example queries from TripClick dataset + adding similar documents/queries from dataset of session to query
\end{itemize}
\subsubsection{Reassessing LLM for Boolean Queries for Systematic Reviews}
\begin{itemize}
    \item In medical domains: a lot of very complex boolean queries for literature reviews
    \item Prior paper: boolean queries for systematic reviews, other reproducibility paper found: their results do not generalize -- but did not use syntax checker, produces wrong results!
    \item Also: did not use guide-documents for prompt engineering
    \item Syntax check - regenerate
    \item Q: Did they use grammar for queries (constrained generation?) json output can reduce performance of LLMs \url{https://arxiv.org/abs/2408.02442}
\end{itemize}
\subsubsection{Revisiting Algorithmic Audits of TikTok: Poor Reproducibility and Short-term Validity of Findings}
\begin{itemize}
    \item TikTok audits are diffictult to reproduce (change of platform/bot banning)
    \item Personalitation stronger for longer videos
    \item Country ID by Proxies
    \item[$\implies$] audits only short-term valid
\end{itemize}
\subsubsection{Beyond Reproducibility: Advancing Zero-shot LLM Reranking Efficiency with Setwise Insertion}
\begin{itemize}
    \item Previous: Comparison by \enquote{Bubble-Sort-by-LLM}
    \item Do chunked reordering using LMs
\end{itemize}
\subsection{Knowledge  \& KGs}
\subsubsection{Segmentation Similarity Enhanced Semantic Related Entity Fusion for Multi-modal Knowledge Graph Completion}
\begin{itemize}
    \item Using ViT+Semantic Segmentation to complete KGs with more multimodal knowledge (location based on images, ...)
    \item Tranformers over Transformers...
\end{itemize}
\subsubsection{Mitigating Modality Bias in Multi-modal Entity Alignment from a Causal Perspective}
\begin{itemize}
    \item finding equality/alignment between KGs using semantic information
\end{itemize}
\subsection{Human and Interfaces}
\subsubsection{The In-Situ Effect of Offensive Ads on Search Engine Users}
\begin{itemize}
    \item people see avg. 5000 ads/day
    \item offensive ads more effective, but at a risk -- evoke negative emotions
    \item complaints: usually only very few offensive ads (Bing: a single offensive ad spreadsheet!)
    \item then: user study to count (offensive) ads 
    \item Result: users can barely distinguish ads, irrelevance and offensiveness is very correlated; irrelevance is considered more offensive than offensive itself!
    \item People do not recall offensive ads on bing/google
\end{itemize}
\subsubsection{Simulating Before Planning: Constructing Intrinsic User World Model for User-Tailored Dialogue Policy Planning}
\begin{itemize}
    \item Agentic framework ...
\end{itemize}

\section{Tuesday}
\subsection{Keynote: Digital Health}
\begin{itemize}
    \item Vaguely related to IR, advertisement for the industrial lab
    \item Prediction of treatment/preselection of people based on disease history
    \item Kenneth Church: Chatbots are not the solution for Callcenters - could be just an app...
\end{itemize}
\subsection{Search and Ranking}
(Peter's favorite session!)
\subsubsection{CoDIME: A Counterfactual Approach for Dimension Importance Estimation through Click Logs}
\begin{itemize}
    \item Dimension Importance Model Estimation (DIME): only keep good dimensions for dense retrieval
    \item Estimate through LLM Gen docs (inconsistent) OR relevant docs from collection (need ground truth)
    \item Solution: use implicit feedback (click data)
    \item Estimate Correlation between rankings/clicks; fit linear model between this correlation (for dimension) as estimator
    \item Improves retrieval significantly - interesting for dense semantic retrieval!
\end{itemize}

\subsubsection{Stitching Inner Product and Euclidean Metrics for Topology-aware Maximum Inner Product Search}
\url{Stitching Inner Product and Euclidean Metrics for Topology-aware Maximum Inner Product Search}
\begin{itemize}
    \item Nearest neighbour search using euclidian distance is not great, as inner poroduct not a natural norm (no triangle ineq); need it for speedup of MIPS
    \item This seems like a convex hull over neighbourhood?
\end{itemize}

\subsubsection{On the Scaling of Robustness and Effectiveness in Dense Retrieval}
\begin{itemize}
    \item Robustness of models largely ignored, lower limit of system relevant (in OOD settings)!
    \item Incorporate Robustness into loss
    \item Danger zone: did they use the test data to compute the robustness in the loss? -- not a great answer in session
    \item Also no comparison to BM25 baseline...
\end{itemize}

\subsubsection{Optimizing Compound Retrieval Systems}
\begin{itemize}
    \item Cascading retrieval systems (rerankers...)
    \item LLM Ranking does not really fit into reranker approach
    \item New paradigm: compound retrieval:\emph{any combination} -- cascading just a specific instance
    \item Propose fully differentiable rerank pipeline - can just learn to design the compound system
    \item Results: dynamic trade-off between LM calls and 
    \begin{itemize}
        \item BM25 ranker for pointwise comparison
        \item Cascading system seems to be good choice
        \item sparse selection of top seems also nice (or even other patterns of selection)
    \end{itemize}
    \item Conclusion: could be interesting for novel compund systems
\end{itemize}
\subsubsection{Hypencoder: Hypernetworks for Information Retrieval}
\begin{itemize}
    \item Problem: the linearity of dot product could result in non-linearly-sperarable subspaces for retrieval
    \item Solution: non-linear query-specific NN using Hypernetworks - learned NN that produces weights for other NNs
\end{itemize}
\subsubsection{Classifying Term Variants in Query Formulation}
\begin{itemize}
    \item Variety of query for the same goal -- 95/100 queries unique even for simple query!
    \item over 10k query formulations for 100 query back stories
    \item Analysis: remove back story; analyse added termns -- users added 70\% terms by themselves
    \item User modify semantics, misspell...
    \item Or enrich terms with information types or sources
\end{itemize}
\subsubsection{Locality-Sensitive Indexing for Graph-Based Approximate Nearest Neighbor Search}
\begin{itemize}
    \item kNN search: graph based, often fsated by searching topologies
    \item Update patterns: reconnection best, deletion/hiding hurt performance (recall/time)
    \item LIGS: bucketize/hash vectors together -- simulated graph by overlaying buckets -- connections only implicit!
    \item good for high-churn applications (social media/networks)
\end{itemize}
\subsubsection{Constrained Auto-Regressive Decoding Constrains Generative Retrieval}
\begin{itemize}
    \item Generative retrieval methods use constrained generative retrieval
    \item Either: step-wise elimination or beam-search over $k$ sampling steps
    \item Result: erroneous retrieval, and will give high precision and low recall (just as we observed in the GutBrainIE evaluation!!)
\end{itemize}
\subsection{Biomedical and Health}
\subsubsection{Cooking with Conversation: Enhancing User Engagement and Learning with a Knowledge-Enhancing Assistant}
\begin{itemize}
    \item Allow users either an active or passive guidance for Cooking
    \item include annotated recipes (KG/facts)
\end{itemize}
\subsubsection{General Neural Embedding for Sequence Distance Approximation}
\begin{itemize}
    \item Usually: Distance metrics like edit distance hard to compute
    \item distance approximated using NN, existing approachs lack generality
    \item Their Approach: Small domain head, shared CNN+Transformer block for all domains!
    \item Show good performance on both DNA and GPS data (??), no significance test
\end{itemize}
\subsubsection{ProtChatGPT: Towards Understanding Proteins with Hybrid Representation and Large Language Models}
\begin{itemize}
    \item PLP-Former for special protein tokens
\end{itemize}
\subsubsection{Combining Evidence and Reasoning for Biomedical Fact-Checking}
\begin{itemize}
    \item 3-stage framework for verification of facts, including justification aoutput of LMs
\end{itemize}
\section{Wednesday}
\subsection{Keynote: Please meet AI, our dear new colleague. In other words: can scientists and machines truly cooperate?}
(Iryna Gurevych)
\begin{itemize}
    \item Peer review assistance through various tools: ACL and AAAI already incorporate this!
    \item Specific Review Datasets
    \item Workflow for Review Process, AI cannot fully arrive at review but help prefill fields
    \item Related works: difficult to write a good argumentative section, not just summarize!
    \item Connection between reviews and revisions -- hard, as the task is quite undefined!
    \item Aspects in review: Detect aspects in fields semi-autonomously (ontology!); LLM generated reviews are very similar w.r.t. aspects! -- use for detection of LLM detection; 
    \item Lazy Review heuristic: i.e. did not do ...
    \item Reviews and promised changes: relate them automatically, verifiy completion!
\end{itemize}
\subsection{Efficiency}
\subsubsection{WebANNS: Fast and Efficient Approximate Nearest Neighbor Search in Web Browsers}
(Peter: nice WASM usage); very good presentation!
\begin{itemize}
    \item Previously: USed IndexedDB within browser / prefetch data points, us HNSW + JS (?)
    \item Proposed: WASM, problem WASM $\rightleftharpoons$ IndexedDB, IndexedDB slow
    \item Tree In-Memory, rest in IndexedDB+JS (three-tier data storage: WASM-JS-DB), Lazyload per tree level
    \item Benchmarks show sub-second performance!
\end{itemize}
\subsubsection{TITE: Token-Independent Text Encoder for Information Retrieval}
\begin{itemize}
    \item Reduce computational cost by aggregrating tree-wise, but the model can increase dimensionality
    \item Related to Funnel Transformers: pool (e.g. $2\times$) over one of the attention dimensions
    \item without loss of performance!
\end{itemize}
\subsubsection{WARP: An Efficient Engine for Multi-Vector Retrieval}
(Best Paper Award)
\begin{itemize}
    \item Combine XTR+ColBERTv2/PLAID to do token-wise retrieval
\end{itemize}
\subsubsection{An Analysis on Matching Mechanisms and Token Pruning for Late-interaction Models}
\begin{itemize}
    \item Prune tokens based on attention/...
    \item Little effiency loss
\end{itemize}
\subsubsection{Data Augmentation for Sample Efficient and Robust Document Ranking}
\begin{itemize}
    \item Augment data by splitting docs \& rerank passages of docs based on simple methods
    \item Solid eval on various loss/augmentation combinations
    \item Augmentation is useful, especially in low-resource
    \item Acts as regularizer, allows transfer from e.g. MSMARCO to other domains
\end{itemize}
\subsection{Reranking}
\subsubsection{Bridging Personalization and Control in Scientific Personalized Search}
\begin{itemize}
    \item Focus: Reranker with personalized search
    \item Cross-Encoder for reranking, their contribution: personalization
    \item Approach: represent history as context to the reranker; add personalization by mixing concepts using 'routing'
    \item Could be interesting to visulize within OnSET to improve user guidance?
\end{itemize}
\subsubsection{Reason-to-Rank: Distilling Direct and Comparative Reasoning from Large Language Models for Document Reranking}
\begin{itemize}
    \item Add reasoning model for reranking model
\end{itemize}
\subsection{Low-Resouce}
\subsubsection{When Less is Enough: Optimizations for Low-Cost Recommendation Systems}
\begin{itemize}
    \item Low-ARPU regions: profitability @ \$4/user
\end{itemize}
\subsubsection{Efficient Approximate Nearest Neighbor Search on a Raspberry Pi}
\subsubsection{IR for AAC Users: A Hyperdimensional Computing (Vector Symbolic Architectures) Approach}
\subsubsection{Some Things Never Change: Overcoming Persistent Challenges in Children IR}
\section{Thursday (Workshops)}
\subsection{ReNeuIR at SIGIR 2025: The Fourth Workshop on Reaching Efficiency in Neural Information Retrieval}

\section{Posters}
\include{posters}

\end{document}